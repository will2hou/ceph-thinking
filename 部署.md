## 搭建Ceph

环境准备：用200G磁盘空间搭建Ceph，用60G的磁盘空间划分osd，每个osd10G

## 准备工作

开始搭建Ceph，先从github（或者gitee）上clone源码，这里选用gitee的源码：[https://gitee.com/ghxandsky/ceph/tree/v15.2.4/](https://link.zhihu.com/?target=https%3A//gitee.com/ghxandsky/ceph/tree/v15.2.4/)

```bash

#从github上克隆
git clone https://github.com/ceph/ceph.git
cd ceph
git checkout v18.1.1 -b 18.1.1
git submodule update --init --recursive
#-b 用来指定git里的分支
```

克隆成功后会在出现Ceph的目录，并cd 到该目录下

## 下载依赖

在正式搭建Ceph之前需要下载一些依赖

```bash
./install-deps.sh
```


构建完成：

## 开始构建Ceph

Ceph 是使用 cmake 构建的。要构建 Ceph，请导航到克隆的 Ceph 存储库并执行以下命令:

官方文档提供的方法：Ceph 用 `automake` 和 `configure` 脚本简化构建过程。先进入刚克隆的 Ceph 源码库，执行下列命令开始构建：

```bash
#本次构建未采用此方法
cd ceph
./do_cmake.sh
cd build
ninja
```

 

安装：

```bash
ninja install
```

## 集群搭建

单节点：一个monitor节点，一个OSD，一个manger

设置时间同步：

```bash
apt-get install ntp
```

配置主机名：

```bash
hostnamectl set-hostname node1

```

配置hosts解析

```bash
vim /etc/hosts

```

多节点的时候需要节点间ssh互信

```bash
ssh-keygen -t rsa 
ssh-copy-id root@node1 
ssh-copy-id root@node2 
ssh-copy-id root@node3

```

## Monitor节点

在node1节点生成uuid作为cluster id：d8cb7c94-154b-4705-9517-12999098544d

```bash
uuidgen
031a63a5-8dd3-498f-9bd4-f50adcce086c

```

export cephuid=d8cb7c94-154b-4705-9517-12999098544d

设置Ceph配置文件 /etc/ceph/ceph.conf 若没有，请自行创建。

```bash
vim  /etc/ceph/ceph.conf

```



```bash
[global]
fsid = 生成的uuid
mon initial members = ${hostname}
mon host =  ${ip_address}
public network = ${ip_address%.*}.0/24
auth cluster required = cephx
auth service required = cephx
auth client required = cephx
osd journal size = 1024
osd pool default size = 3
osd pool default min size = 1
osd pool default pg num = 128
osd pool default pgp num = 128
osd crush chooseleaf type = 1
mon allow pool delete = true

```

参数说明：

 fsid：这个就是刚才生成的集群唯一uuid

 mon initial members：这个配置监视器的主机名列表，多个用逗号分隔

 mon host：这个配置监视器节点的ip:port列表，默认ceph-mon服务的端口号是6789，默认不修改可以不写，多个用逗号分隔

 public network: 表示开放客户端访问的网络段，根据实际的情况配置

 然后后面3项均表示启动认证，方式就是cephx

 然后重点看：osd pool default size和osd pool default min size，第一个是osd的数据会复制多少份，osd节点或服务的个数至少要>=复制份数，正常生产环境副本至少要设置3个，保证数据安全，我们这里就两个osd，因此最多设置2；然后后面的配置是允许降级状态下写入几个副本，通常建议至少配置2，我们这里只有两个osd，因此配置了1

 然后是osd pool default pg num和osd pool default pgp num是配置单个pool默认的pg数量和pgp数量，pg全称是Placement Group，叫放置组，也就数据存储分组的单元，可以类比为hdfs的块类似的概念，pgp num要和pg num一致即可

 osd crush chooseleaf type这个参数要注意，这个默认是1表示不允许把数据的不同副本放到1个节点上，这样是为了保证数据安全，集群肯定要配置成1，如果是单机环境搭多个osd做伪分布式测试，那么副本肯定是都在本机的，这个参数务必要改成0，否则最后pgs一直达不到active+clean的状态，一定要注意这个参数

 mon_allow_pool_delete 这个参数表示是否允许删除pool，就是存储池，默认是不允许的，改成这个则允许删除，生产环境建议关闭

 上面就是基本的这些配置，然后需要把这个配置文件同步到所有的节点，注意之后ceph.conf只要有任何修改，都要同步到集群全部节点，不管是否用到这些配置项，目的是为了保证集群的配置统一，避免出现特殊的问题

为集群创建一个keyring，并生成一个monitor密钥。

```bash
ceph-authtool --create-keyring /tmp/ceph.mon.keyring --gen-key -n mon. --cap mon 'allow *'

```

生成administrator keyring，生成`client.admin`用户并将用户添加到keyring。

```bash
ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *'

```

生成bootstrap-osd keyring，生成`client.bootstrap-osd`用户并将用户添加到keyring。

```bash
#目录可能没有，先创建目录
mkdir -p /var/lib/ceph/bootstrap-osd

ceph-authtool --create-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring --gen-key -n client.bootstrap-osd --cap mon 'profile bootstrap-osd' --cap mgr 'allow r'

```

将生成的密钥添加到中`ceph.mon.keyring`。

```bash
ceph-authtool /tmp/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring
ceph-authtool /tmp/ceph.mon.keyring --import-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring

```

*将所有者更改为`ceph.mon.keyring`。*

```bash
chown ceph:ceph /tmp/ceph.mon.keyring

```

使用主机名，主机IP地址和FSID生成monitor map。另存为`/tmp/monmap`：

**如果是多节点还需要以下操作：**

复制monitor map到另外2个节点

```bash
scp /tmp/monmap root@node2:/tmp
scp /tmp/monmap root@node3:/tmp

```

复制ceph.client.admin.keyring到另外2个节点

```bash
scp /etc/ceph/ceph.client.admin.keyring root@node2:/etc/ceph/
scp /etc/ceph/ceph.client.admin.keyring root@node3:/etc/ceph/

```

复制ceph.mon.keyring到另外2个节点

```bash
scp /tmp/ceph.mon.keyring root@node2:/tmp/
scp /tmp/ceph.mon.keyring root@node3:/tmp/

#注意修改文件权限
[root@node2 ~]# chown ceph:ceph /tmp/ceph.mon.keyring
[root@node3 ~]# chown ceph:ceph /tmp/ceph.mon.keyring

```

创建monitor数据目录

```bash
mkdir -p /var/lib/ceph/mon/ceph-${hostname}

```

用monitor map和keyring填充monitor守护程序

```bash
#若是多个节点 需要在每个节点都进行此操作
#node1
ceph-mon --mkfs -i node1 --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring

```



启用monitor服务

```bash
#建一个空文件 done ，表示监视器已创建、可以启动了
touch /var/lib/ceph/mon/ceph-node1/done
ceph-mon -i node1

```

查看集群状态

```bash
ceph -s

```

存在health告警，启用msgr2后会消失。

```bash
ceph-mon enable-msgr2

```

Manger节点

```bash
ceph auth get-or-create mgr.${mgr_name} mon 'allow *' osd 'allow *' mds 'allow *'
#创建manger节点目录
mkdir -p /var/lib/ceph/mgr/ceph-${mgr_name}
ceph auth get mgr.${mgr_name} -o  /var/lib/ceph/mgr/ceph-${mgr_name}/keyring

```

启用Manger节点

```bash
ceph-mgr -i mgr1

```

## OSD节点

本次选用sdc下的1 2 3分别做osd

使用ceph-volume准备osd，其中/dev/sdc1 更换为要做osd的磁盘

```bash
ceph-volume lvm prepare --data /dev/sdc1
#ceph-volume lvm prepare --data /dev/sdc

```

![img](https://pic2.zhimg.com/80/v2-cbd364fb379fe1bd54a8fd387277a1e5_720w.webp)

启用osd

```text
ceph-osd -i 0
#osd_name

```

![img](https://pic3.zhimg.com/80/v2-8a0d519d13cee1d2802b88517a747d3e_720w.webp)



## 删除集群

```text
ps aux|grep ceph |awk '{print $2}'|xargs kill -9
ps -ef|grep ceph
```

**确保此时所有ceph进程都已经关闭！！！如果没有关闭，多执行几次**

```bash
umount /var/lib/ceph/osd/*
rm -rf /var/lib/ceph/osd/*
rm -rf /var/lib/ceph/mon/*
rm -rf /var/lib/ceph/mds/*
rm -rf /var/lib/ceph/bootstrap-mds/*
rm -rf /var/lib/ceph/bootstrap-osd/*
rm -rf /var/lib/ceph/bootstrap-rgw/*
rm -rf /var/lib/ceph/bootstrap-mgr/*
rm -rf /var/lib/ceph/tmp/*
rm -rf /var/log/ceph/*
rm -rf /tmp/ceph*
rm -rf /etc/ceph/*
rm -rf /var/run/ceph/*
#rm -rf /tmp/monmap
```
